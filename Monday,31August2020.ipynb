{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Monday,31August2020.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN9XZ3USMpLob2HkwW11lPn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayush5harma/IT-603/blob/master/Monday%2C31August2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ4ZOpRnPTYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rZWTlvBQ73G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3cad5219-67ce-4df4-e078-cbd2db6f542b"
      },
      "source": [
        "######### Word Tokenize ##################\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Splitting words in a sentence using the word_tokenize function which is a wrapper function that calls tokenize on an instance of the TreebankWordTokenizer class.\"\n",
        "print(word_tokenize(text))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Splitting', 'words', 'in', 'a', 'sentence', 'using', 'the', 'word_tokenize', 'function', 'which', 'is', 'a', 'wrapper', 'function', 'that', 'calls', 'tokenize', 'on', 'an', 'instance', 'of', 'the', 'TreebankWordTokenizer', 'class', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwZK__TlQ-OY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "19c78f52-46ef-4d4d-aa79-be162ea5ecff"
      },
      "source": [
        "######### Sentence Tokenize ##################\n",
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"Splitting sentences in the paragraph. The sent_tokenize function uses an instance of PunktSentenceTokenizer from the nltk.tokenize.punkt module, which is already been trained and thus very well knows to mark the end and beginning of sentence at what characters and punctuation.\"\n",
        "print(sent_tokenize(text))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Splitting sentences in the paragraph.', 'The sent_tokenize function uses an instance of PunktSentenceTokenizer from the nltk.tokenize.punkt module, which is already been trained and thus very well knows to mark the end and beginning of sentence at what characters and punctuation.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnmxyt31XDc6",
        "colab_type": "text"
      },
      "source": [
        "**StopWords**\n",
        "\n",
        "\n",
        "\n",
        "> Stopwords are the words in any language which does not add much meaning to a sentence.\n",
        "\n",
        "\n",
        "They can safely be ignored without sacrificing the meaning of the sentence. For some search engines, \n",
        "these are some of the most common, short function words, such as the, is, at, which, and on. In this case,\n",
        "stop words can cause problems when searching for phrases that include them, particularly in names such as\n",
        "“The Who” or “Take That”."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIUKSelGdODq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "356c7e63-ad8b-4b0c-f41d-9446ddaef743"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUj0cUWgRAsy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "68fd549f-863b-4134-b77d-95eed33d3be5"
      },
      "source": [
        "print(stopwords.words('english'))\n",
        "\n",
        "# random sentecnce with lot of stop words\n",
        "sample_text = \"Oh man, this is pretty cool. We will do more such things.\"\n",
        "text_tokens = word_tokenize(sample_text)\n",
        "\n",
        "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words('english')]\n",
        "\n",
        "print(text_tokens)\n",
        "print(tokens_without_sw)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "['Oh', 'man', ',', 'this', 'is', 'pretty', 'cool', '.', 'We', 'will', 'do', 'more', 'such', 'things', '.']\n",
            "['Oh', 'man', ',', 'pretty', 'cool', '.', 'We', 'things', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUXZrNMFRDDh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cb13b34d-7c9d-4f83-b337-f5e0f8fc2a72"
      },
      "source": [
        "sentence=\"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n",
        "porter.stem(sentence)\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "def stemSentence(sentence):\n",
        "    token_words=word_tokenize(sentence)\n",
        "    token_words\n",
        "    stem_sentence=[]\n",
        "    for word in token_words:\n",
        "        stem_sentence.append(porter.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    return \"\".join(stem_sentence)\n",
        "\n",
        "x=stemSentence(sentence)\n",
        "print(x)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python are veri intellig and work veri pythonli and now they are python their way to success . \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEZC9TRkRF9F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "5dc154c2-1b08-438d-e7d4-931661ae4053"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
        "punctuations=\"?:!.,;\"\n",
        "sentence_words = nltk.word_tokenize(sentence)\n",
        "for word in sentence_words:\n",
        "    if word in punctuations:\n",
        "        sentence_words.remove(word)\n",
        "\n",
        "sentence_words\n",
        "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
        "for word in sentence_words:\n",
        "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word                Lemma               \n",
            "He                  He                  \n",
            "was                 wa                  \n",
            "running             running             \n",
            "and                 and                 \n",
            "eating              eating              \n",
            "at                  at                  \n",
            "same                same                \n",
            "time                time                \n",
            "He                  He                  \n",
            "has                 ha                  \n",
            "bad                 bad                 \n",
            "habit               habit               \n",
            "of                  of                  \n",
            "swimming            swimming            \n",
            "after               after               \n",
            "playing             playing             \n",
            "long                long                \n",
            "hours               hour                \n",
            "in                  in                  \n",
            "the                 the                 \n",
            "Sun                 Sun                 \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}