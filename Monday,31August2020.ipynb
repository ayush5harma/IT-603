{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Monday,31August2020.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNvd7xLlu7zjgPBmUt+Bsov",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayush5harma/IT-603/blob/master/Monday%2C31August2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWbs3XuQfVSv",
        "colab_type": "text"
      },
      "source": [
        "**Tokenize Words and Sentences with NLTK**\n",
        "\n",
        "Tokenization is the process by which big quantity of text is divided into smaller parts called tokens.\n",
        "These tokens are useful for finding such patterns as well as is considered as a base step for stemming and lemmatization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sywbNB69fq7P",
        "colab_type": "text"
      },
      "source": [
        "**Tokenization of words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MH5hNAEVf7uy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rZWTlvBQ73G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9c384ba3-7324-4632-af1c-6864b0aff3d7"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Splitting words in a sentence using the word_tokenize function which is a wrapper function that calls tokenize on an instance of the TreebankWordTokenizer class.\"\n",
        "print(word_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Splitting', 'words', 'in', 'a', 'sentence', 'using', 'the', 'word_tokenize', 'function', 'which', 'is', 'a', 'wrapper', 'function', 'that', 'calls', 'tokenize', 'on', 'an', 'instance', 'of', 'the', 'TreebankWordTokenizer', 'class', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT7fgPsYfwAg",
        "colab_type": "text"
      },
      "source": [
        "**Tokenization of Sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwZK__TlQ-OY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a2d53c0e-5f2f-4d89-8da2-f7f0fe420a30"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"Splitting sentences in the paragraph. The sent_tokenize function uses an instance of PunktSentenceTokenizer from the nltk.tokenize.punkt module, which is already been trained and thus very well knows to mark the end and beginning of sentence at what characters and punctuation.\"\n",
        "print(sent_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Splitting sentences in the paragraph.', 'The sent_tokenize function uses an instance of PunktSentenceTokenizer from the nltk.tokenize.punkt module, which is already been trained and thus very well knows to mark the end and beginning of sentence at what characters and punctuation.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnmxyt31XDc6",
        "colab_type": "text"
      },
      "source": [
        "**StopWords**\n",
        "\n",
        "\n",
        "\n",
        "> Stopwords are the words in any language which does not add much meaning to a sentence.\n",
        "\n",
        "\n",
        "They can safely be ignored without sacrificing the meaning of the sentence. For some search engines, \n",
        "these are some of the most common, short function words, such as the, is, at, which, and on. In this case,\n",
        "stop words can cause problems when searching for phrases that include them, particularly in names such as\n",
        "“The Who” or “Take That”."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIUKSelGdODq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "94a3f5bd-5f34-470e-df3b-5987aa2193a7"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUj0cUWgRAsy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "e874c6ae-c609-4327-e31a-c50a82f16ecb"
      },
      "source": [
        "print(stopwords.words('english'))\n",
        "\n",
        "# random sentecnce with lot of stop words\n",
        "sample_text = \"Oh man, this is pretty cool. We will do more such things.\"\n",
        "text_tokens = word_tokenize(sample_text)\n",
        "\n",
        "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words('english')]\n",
        "\n",
        "print(text_tokens)\n",
        "print(tokens_without_sw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "['Oh', 'man', ',', 'this', 'is', 'pretty', 'cool', '.', 'We', 'will', 'do', 'more', 'such', 'things', '.']\n",
            "['Oh', 'man', ',', 'pretty', 'cool', '.', 'We', 'things', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI3eV1jomcCV",
        "colab_type": "text"
      },
      "source": [
        "**Stemming** \n",
        "\n",
        "> Stemming is basically removing the suffix from a word and reduce it to its root word.\n",
        "\n",
        "For example: “Flying” is a word and its suffix is “ing”, if we remove “ing” from “Flying” then we will get base word or root word which is “Fly”.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEMscXGKmdja",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b92bdf51-d539-428c-cb42-2782f0f8b8c3"
      },
      "source": [
        "from nltk.stem.porter import *\n",
        "\n",
        "porterStemmer = PorterStemmer()\n",
        "\n",
        "sentence=\"I'm breakable; Unbreakable I'm shaking yet Unshakable Until the day that you find me I'll stand here Existing and feeling wretched existence Consuming life-force 'til I grow distant Don't bother searching for somebody like me A fading no one I don't want to hurt you, it's not my nature A monster born from dusk to dawn can't be your saviour Remember the 'me', the way I used to be\"\n",
        "wordList = nltk.word_tokenize(sentence)\n",
        "\n",
        "stemWords = [porterStemmer.stem(word) for word in wordList]\n",
        "\n",
        "print(' '.join(stemWords))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I 'm breakabl ; unbreak I 'm shake yet unshak until the day that you find me I 'll stand here exist and feel wretch exist consum life-forc 'til I grow distant Do n't bother search for somebodi like me A fade no one I do n't want to hurt you , it 's not my natur A monster born from dusk to dawn ca n't be your saviour rememb the 'me ' , the way I use to be\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUztcKdphnRo",
        "colab_type": "text"
      },
      "source": [
        " **Lemmatization**\n",
        " \n",
        "Both in stemming and in lemmatization, we try to reduce a given word to its root word. The root word is called a stem in the stemming process, and it is called a lemma in the lemmatization process.\n",
        "In lemmatization,the algorithms refer a dictionary to understand the meaning of the word before reducing it to its root word, or lemma."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEZC9TRkRF9F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "5dc154c2-1b08-438d-e7d4-931661ae4053"
      },
      "source": [
        "#lemmatize excluding verbs\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
        "punctuations=\"?:!.,;\"\n",
        "sentence_words = nltk.word_tokenize(sentence)\n",
        "for word in sentence_words:\n",
        "    if word in punctuations:\n",
        "        sentence_words.remove(word)\n",
        "\n",
        "sentence_words\n",
        "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
        "for word in sentence_words:\n",
        "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word                Lemma               \n",
            "He                  He                  \n",
            "was                 wa                  \n",
            "running             running             \n",
            "and                 and                 \n",
            "eating              eating              \n",
            "at                  at                  \n",
            "same                same                \n",
            "time                time                \n",
            "He                  He                  \n",
            "has                 ha                  \n",
            "bad                 bad                 \n",
            "habit               habit               \n",
            "of                  of                  \n",
            "swimming            swimming            \n",
            "after               after               \n",
            "playing             playing             \n",
            "long                long                \n",
            "hours               hour                \n",
            "in                  in                  \n",
            "the                 the                 \n",
            "Sun                 Sun                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u64-_H90p1lS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "outputId": "0d67bf03-bb27-4f37-afa6-4d2129d485e3"
      },
      "source": [
        "#lemmatize including verbs with pos_tag function\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "txt = \"\"\"Resumption of the session I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999 , and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period .\"\"\"\n",
        "[wnl.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else wnl.lemmatize(i) for i,j in pos_tag(word_tokenize(txt))]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Resumption',\n",
              " 'of',\n",
              " 'the',\n",
              " 'session',\n",
              " 'I',\n",
              " 'declare',\n",
              " 'resume',\n",
              " 'the',\n",
              " 'session',\n",
              " 'of',\n",
              " 'the',\n",
              " 'European',\n",
              " 'Parliament',\n",
              " 'adjourn',\n",
              " 'on',\n",
              " 'Friday',\n",
              " '17',\n",
              " 'December',\n",
              " '1999',\n",
              " ',',\n",
              " 'and',\n",
              " 'I',\n",
              " 'would',\n",
              " 'like',\n",
              " 'once',\n",
              " 'again',\n",
              " 'to',\n",
              " 'wish',\n",
              " 'you',\n",
              " 'a',\n",
              " 'happy',\n",
              " 'new',\n",
              " 'year',\n",
              " 'in',\n",
              " 'the',\n",
              " 'hope',\n",
              " 'that',\n",
              " 'you',\n",
              " 'enjoy',\n",
              " 'a',\n",
              " 'pleasant',\n",
              " 'festive',\n",
              " 'period',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}